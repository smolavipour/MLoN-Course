{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import random\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('data/household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train.T, X_test.T, y_train, y_test, no_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "X size (6, 1536960)\n",
      "Y size (1536960,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\\begin{align}\n",
    "    E=\\min_{w_3, W_2,W_1} \\frac{1}{N}\\sum_i || w_3 s(W_2 s(W_1 x_i)-y_i)||^2   \n",
    "\\end{align}\n",
    "\n",
    "## Layer 3\n",
    "Define \n",
    "\\begin{align}\n",
    "a_3(x)& :=w_3 s(W_2 s(W_1 x))\\\\\n",
    "a_2(x)& :=s(W_2 s(W_1 x)),\\\\\n",
    "a_1(x)& :=s(W_1 x).\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_3} &= \\frac{2}{N}(a_3-t)\\frac{\\partial a_3}{\\partial w_3} \\\\\n",
    "&=\\frac{2}{N}(x_3-t)\\frac{\\partial w_3a_2}{\\partial w_3}\\\\\n",
    "&=\\frac{2}{N}(x_3-t)a_2^T\\\\\n",
    "\\end{align}\n",
    "\n",
    "So defining \n",
    "$$\\delta_3 := \\frac{2}{N}(a_3-t),$$\n",
    "then\n",
    "$$\\frac{\\partial E}{\\partial w_3} =\\delta_3\\,a_2^T.$$\n",
    "\n",
    "## Layer 2\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_2} &= \\frac{2}{N}(a_3-t)\\frac{\\partial a_3}{\\partial W_2} \\\\\n",
    "&=\\frac{2}{N}(a_3-t)\\frac{\\partial (W_3 a_2)}{\\partial W_2}\\\\\n",
    "&=\\delta_3\\frac{\\partial (W_3 a_2)}{\\partial W_2}\\\\\n",
    "&=W_3^T\\delta_3\\frac{\\partial a_2}{\\partial W_2}\\\\\n",
    "&=[W_3^T\\delta_3 \\circ s'(W_2 a_1)]\\frac{\\partial W_2 a_1}{\\partial W_2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So defining $$\\delta_2 :=W_3^T\\delta_3 \\circ s'(W_2 a_1),$$\n",
    "we have\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W_2}=\\delta_2 a_1^T$$\n",
    "\n",
    "## Layer 1\n",
    "\n",
    "Define \n",
    "$$\\delta_1 :=W_2^T\\delta_2 \\circ s'(W_1x),$$\n",
    "similar to layer_2:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_1} &=\\delta_1x^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    #X is q x n\n",
    "    # w_1 is p x q\n",
    "    # w_2 is p x p\n",
    "    # w_3 is 1 x p\n",
    "    layer_0=X # q x n\n",
    "    layer_1=sigmoid(np.matmul(w_1 , X)) # p x n \n",
    "    layer_2=sigmoid(np.matmul(w_2 , layer_1)) # p x n \n",
    "    layer_3=np.matmul(w_3 , layer_2) # p x n\n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "\n",
    "\n",
    "# Fill in backpropagation    \n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3):\n",
    "    # Calculate the gradient here\n",
    "    N = y.shape[0]    \n",
    "        \n",
    "    delta3=2/N*(layer_3 - y)\n",
    "    delta2=np.multiply(np.matmul(w_3.T,delta3),sigmoid(np.matmul(w_2,layer_1),derivative=True))\n",
    "    delta1=np.multiply(np.matmul(w_2.T,delta2),sigmoid(np.matmul(w_1,layer_0),derivative=True))\n",
    "    \n",
    "    layer_3_delta=np.matmul(delta3,layer_2.T)\n",
    "    layer_2_delta=np.matmul(delta2,layer_1.T)\n",
    "    layer_1_delta=np.matmul(delta1,layer_0.T)\n",
    "\n",
    "    return layer_1_delta, layer_2_delta, layer_3_delta\n",
    "\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Funtion to get mini batch sgd\n",
    "def miniBatch(x,y,batchSize):\n",
    "    D,N = x.shape\n",
    "    X_mini = np.zeros((D,batchSize))\n",
    "    Y_mini = np.zeros((batchSize,))\n",
    "    indexArray = random.sample(range(N), batchSize)\n",
    "    for i in range(batchSize):\n",
    "        X_mini[:,i] = x[:,indexArray[i]]\n",
    "        Y_mini[i,] = y[indexArray[i],]\n",
    "    return X_mini,Y_mini\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "    cost_l=[]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        X_mini,Y_mini = miniBatch(X,y,batch_size)\n",
    "        L0,L1,L2,L3 = feed_forward_propagation(X_mini,Y_mini,w_1,w_2,w_3,lmbda)\n",
    "        D1,D2,D3 = back_propagation(Y_mini,w_1,w_2,w_3,L0,L1,L2,L3)\n",
    "\n",
    "        #cost1 = cost(X_mini, Y_mini, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "        a = w_1-(learning_rate*D1).reshape(w_1.shape)\n",
    "        b = w_2-(learning_rate*D2).reshape(w_2.shape)\n",
    "        c = w_3-(learning_rate*D3).reshape(w_3.shape)\n",
    "        \n",
    "        #cost2 = cost(X_mini, Y_mini, a, b, c, lmbda)\n",
    "    \n",
    "        #if ((cost2-cost1)/cost1>0.5):\n",
    "        #    break\n",
    "        \n",
    "        w_1 = a\n",
    "        w_2 = b\n",
    "        w_3 = c\n",
    "                \n",
    "        cost_l.append(cost(X,y,w_1,w_2,w_3,lmbda=lmbda))\n",
    "        #print(i,': ', cost_l[-1])\n",
    "    return w_1, w_2, w_3, cost_l\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T,M,iterations):\n",
    "    #M is the numebr of samples used in the minibatch\n",
    "    cost_l=[]    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        K  = floor(iterations/T)\n",
    "        N  = X.shape[1]\n",
    "        wk_1= w_1\n",
    "        wk_2= w_2\n",
    "        wk_3= w_3\n",
    "        \n",
    "        for k in range(K):\n",
    "            L0,L1,L2,L3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "            ga_1, ga_2, ga_3 = back_propagation(y,wk_1,wk_2,wk_3,L0,L1,L2,L3) #the average\n",
    "            \n",
    "            for t in range(T):\n",
    "                index = np.random.randint(N, size=M)\n",
    "                L0,L1,L2,L3 = feed_forward_propagation(X[:,index],y[index,],w_1,w_2,w_3,lmbda)\n",
    "                g1_1,g1_2,g1_3 = back_propagation(y[index,], w_1,w_2,w_3,L0,L1,L2,L3)\n",
    "                \n",
    "                Lk0,Lk1,Lk2,Lk3 = feed_forward_propagation(X[:,index],y[index,],wk_1,wk_2,wk_3,lmbda)\n",
    "                g2_1,g2_2,g2_3 = back_propagation(y[index,], wk_1,wk_2,wk_3,Lk0,Lk1,Lk2,Lk3)\n",
    "                \n",
    "                g1  = g1_1 - g2_1 + ga_1\n",
    "                g2  = g1_2 - g2_2 + ga_2\n",
    "                g3  = g1_3 - g2_3 + ga_3\n",
    "\n",
    "                #cost1 = cost(X, y, w_1, w_2, w_3, lmbda)\n",
    "            \n",
    "                w_1 = w_1 - (learning_rate*g1).reshape(w_1.shape)\n",
    "                w_2 = w_2 - (learning_rate*g2).reshape(w_2.shape)\n",
    "                w_3 = w_3 - (learning_rate*g3).reshape(w_3.shape)\n",
    "            \n",
    "\n",
    "            wk_1 = w_1\n",
    "            wk_2 = w_2\n",
    "            wk_3 = w_3\n",
    "        cost_l.append(cost(X,y,w_1,w_2,w_3,lmbda=lmbda))    \n",
    "        #print(i,': ', cost_l[-1])        \n",
    "    return w_1, w_2, w_3, cost_l\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    cost_l=[]\n",
    "    for i in range(iterations): \n",
    "        \n",
    "        L0,L1,L2,L3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        D1,D2,D3 = back_propagation(y,w_1,w_2,w_3,L0,L1,L2,L3)\n",
    "    \n",
    "        #cost1 = cost(X, y, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "        w_1 = w_1-(learning_rate*D1).reshape(w_1.shape)\n",
    "        w_2 = w_2-(learning_rate*D2).reshape(w_2.shape)\n",
    "        w_3 = w_3-(learning_rate*D3).reshape(w_3.shape)\n",
    "        \n",
    "        cost_l.append(cost(X,y,w_1,w_2,w_3,lmbda=lmbda))\n",
    "        #print(i,': ', cost_l[-1])        \n",
    "    \n",
    "    return w_1, w_2, w_3, cost_l\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations, noise):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = initialize_w(3,X_train.shape[0])\n",
    "w_2 = initialize_w(3,3)\n",
    "w_3 = initialize_w(1,3)\n",
    "lmbda=0.1\n",
    "print(w_1.shape)\n",
    "print(w_2.shape)\n",
    "print(w_3.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "initialCost=cost(X_train,y_train,w_1,w_2,w_3,lmbda)\n",
    "layer_0,layer_1,layer_2,layer_3 = feed_forward_propagation(X_train,y_train,w_1,w_2,w_3,lmbda)\n",
    "#print('cost: ',costx)\n",
    "\n",
    "w_1_GD,w_2_GD,w_3_GD,cost_l = GD(X_train, y_train, w_1,w_2,w_3, learning_rate = 0.1, lmbda=lmbda, iterations=10)\n",
    "finalCost=cost(X_train,y_train,w_1_GD,w_2_GD,w_3_GD,lmbda)\n",
    "print('Initial Cost:',initialCost)\n",
    "print('Initial Cost:',finalCost)\n",
    "#d1,d2,d3 = back_propagation(y_train, w_1, w_2, w_3, layer_0,layer_1,layer_2,layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_1 = initialize_w(3,X_train.shape[0])\n",
    "w_2 = initialize_w(3,3)\n",
    "w_3 = initialize_w(1,3)\n",
    "lmbda=0.1\n",
    "#SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "initialCost = cost(X_train,y_train,w_1,w_2,w_3,lmbda=0.1)\n",
    "w_1_SGD,w_2_SGD,w_3_SGD,cost_l = SGD(X_train, y_train, w_1, w_2, w_3, lmbda=0.1, learning_rate=0.05,batch_size=100,iterations=20)\n",
    "finalCost = cost(X_train,y_train,w_1_SGD,w_2_SGD,w_3_SGD,lmbda=0.1)\n",
    "print('Initial cost:',initialCost)\n",
    "print('Final cost:',finalCost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = initialize_w(3,X_train.shape[0])\n",
    "w_2 = initialize_w(3,3)\n",
    "w_3 = initialize_w(1,3)\n",
    "initialCost = cost(X_train,y_train,w_1,w_2,w_3,lmbda=0.1)\n",
    "print('Initial cost:',initialCost)\n",
    "s1,s2,s3 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda=0.1, learning_rate=0.05,T=2,M=100,iterations=10)\n",
    "finalCost = cost(X_train,y_train,s1,s2,s3,lmbda=0.1)\n",
    "print('Initial cost:',initialCost)\n",
    "print('Final cost:',finalCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: 593458.257628264\n",
      "GD: 330177.1493233919\n",
      "SGD: 347599.5229124606\n",
      "SGD: 315453.38908577844\n",
      "SGD: 335218.74402868573\n",
      "SVRG: 272895.24425845064\n",
      "SVRG: 272897.9345898819\n",
      "SVRG: 272900.86997842096\n",
      "SVRG: 272900.489568686\n",
      "SVRG: 272907.1906321781\n",
      "SVRG: 272900.4409631461\n",
      "SVRG: 272968.2296752951\n",
      "SVRG: 272915.7694902402\n",
      "SVRG: 272899.80859681574\n",
      "GD: 256174.73857817857\n",
      "SGD: 256159.92059735706\n",
      "SGD: 256463.3554553511\n",
      "SGD: 256177.6200538317\n",
      "SVRG: 256339.90861288874\n",
      "SVRG: 256339.62635491753\n",
      "SVRG: 256339.66355080425\n",
      "SVRG: 256335.5450042203\n",
      "SVRG: 256340.90691920547\n",
      "SVRG: 256338.74372062043\n",
      "SVRG: 256334.92999192805\n",
      "SVRG: 256338.8126332441\n",
      "SVRG: 256339.96085549463\n",
      "GD: 256344.10816724575\n",
      "SGD: 257446.7989660696\n",
      "SGD: 256566.9924871412\n",
      "SGD: 256166.17386268298\n",
      "SVRG: 256623.01092948936\n",
      "SVRG: 256618.4432466093\n",
      "SVRG: 256618.41552125066\n",
      "SVRG: 256615.06833699936\n",
      "SVRG: 256621.42183625372\n",
      "SVRG: 256618.86259861043\n",
      "SVRG: 256643.8932451836\n",
      "SVRG: 256598.3398834743\n",
      "SVRG: 256619.44807474685\n",
      "GD: 330177.1493233919\n",
      "SGD: 299289.53113172826\n",
      "SGD: 332823.7502330676\n",
      "SGD: 328561.5171490503\n",
      "SVRG: 272900.6154927925\n",
      "SVRG: 272902.9579417026\n",
      "SVRG: 272901.2817104061\n",
      "SVRG: 272936.99761630275\n",
      "SVRG: 272909.6022469808\n",
      "SVRG: 272901.406884968\n",
      "SVRG: 272886.71728246886\n",
      "SVRG: 272903.90016192355\n",
      "SVRG: 272902.3944520778\n",
      "GD: 256174.73857817857\n",
      "SGD: 256699.71185282225\n",
      "SGD: 256161.42571704797\n",
      "SGD: 256422.04536082235\n",
      "SVRG: 256341.68478703502\n",
      "SVRG: 256339.91923041557\n",
      "SVRG: 256339.4822479204\n",
      "SVRG: 256344.30319687724\n",
      "SVRG: 256341.50810661924\n",
      "SVRG: 256339.90815787963\n",
      "SVRG: 256340.7843841768\n",
      "SVRG: 256342.61099163478\n",
      "SVRG: 256340.51118941736\n",
      "GD: 256344.10816724575\n",
      "SGD: 256163.64059146473\n",
      "SGD: 257197.0337888248\n",
      "SGD: 256283.69749624\n",
      "SVRG: 256619.34837298022\n",
      "SVRG: 256617.63917776113\n",
      "SVRG: 256621.46908860895\n",
      "SVRG: 256611.0588581049\n",
      "SVRG: 256629.181193037\n",
      "SVRG: 256620.30535725682\n",
      "SVRG: 256736.17017160883\n",
      "SVRG: 256656.3688217909\n",
      "SVRG: 256624.6385301758\n",
      "GD: 330177.1493233919\n",
      "SGD: 317912.9578907085\n",
      "SGD: 327784.05467421754\n",
      "SGD: 331661.3159463776\n",
      "SVRG: 272900.6822208547\n",
      "SVRG: 272902.4838000696\n",
      "SVRG: 272902.03226365964\n",
      "SVRG: 272902.20123846346\n",
      "SVRG: 272915.56275433436\n",
      "SVRG: 272903.8371276869\n",
      "SVRG: 272862.43752092775\n",
      "SVRG: 272880.9684639442\n",
      "SVRG: 272896.55510209076\n",
      "GD: 256174.73857817857\n",
      "SGD: 266624.9977991353\n",
      "SGD: 257951.38572213752\n",
      "SGD: 256294.51567270057\n",
      "SVRG: 256340.54883578283\n",
      "SVRG: 256339.62002409835\n",
      "SVRG: 256339.38621873493\n",
      "SVRG: 256336.94965799744\n",
      "SVRG: 256343.0575150205\n",
      "SVRG: 256339.43326225816\n",
      "SVRG: 256336.09363796876\n",
      "SVRG: 256341.342730514\n",
      "SVRG: 256339.16595942192\n",
      "GD: 256344.10816724575\n",
      "SGD: 268947.21453774354\n",
      "SGD: 261503.4157830129\n",
      "SGD: 256276.93398225657\n",
      "SVRG: 256618.87998794534\n",
      "SVRG: 256618.96883510612\n",
      "SVRG: 256620.4117820888\n",
      "SVRG: 256608.42049525955\n",
      "SVRG: 256603.35478496784\n",
      "SVRG: 256620.64131988157\n",
      "SVRG: 256628.6443621555\n",
      "SVRG: 256640.4513432463\n",
      "SVRG: 256621.49084057202\n",
      "init: 405043.72973778844\n",
      "GD: 290667.2232265735\n",
      "SGD: 301002.22356283455\n",
      "SGD: 282912.1375507753\n",
      "SGD: 289930.2863022662\n",
      "SVRG: 275415.8787044676\n",
      "SVRG: 275410.635465019\n",
      "SVRG: 275414.9417713036\n",
      "SVRG: 275405.29669336864\n",
      "SVRG: 275425.8867508793\n",
      "SVRG: 275411.063489436\n",
      "SVRG: 275457.5992720021\n",
      "SVRG: 275409.2144615117\n",
      "SVRG: 275419.4838413877\n",
      "GD: 281245.58254932665\n",
      "SGD: 271032.04112431785\n",
      "SGD: 270496.08801873965\n",
      "SGD: 284074.77528109174\n",
      "SVRG: 318642.8102574241\n",
      "SVRG: 318630.2182366343\n",
      "SVRG: 318638.16546722926\n",
      "SVRG: 318150.69906342047\n",
      "SVRG: 319016.5429301827\n",
      "SVRG: 318519.9326436087\n",
      "SVRG: 316959.66835107707\n",
      "SVRG: 319188.38831686764\n",
      "SVRG: 318512.9100343264\n",
      "GD: 318455.46652764676\n",
      "SGD: 310289.3555407317\n",
      "SGD: 339683.5236721973\n",
      "SGD: 309584.2021210567\n",
      "SVRG: 423070.090783736\n",
      "SVRG: 423554.74543159833\n",
      "SVRG: 423348.75703722\n",
      "SVRG: 424637.34954671765\n",
      "SVRG: 426481.66718965257\n",
      "SVRG: 423284.0303624649\n",
      "SVRG: 424765.18717067613\n",
      "SVRG: 422074.85373945493\n",
      "SVRG: 423807.6984441865\n",
      "GD: 290667.2232265735\n",
      "SGD: 292489.030511758\n",
      "SGD: 300790.1482239529\n",
      "SGD: 290581.2844314797\n",
      "SVRG: 275419.18962649564\n",
      "SVRG: 275412.6236827742\n",
      "SVRG: 275416.54369915515\n",
      "SVRG: 275415.5372093329\n",
      "SVRG: 275420.43726033834\n",
      "SVRG: 275413.63645174954\n",
      "SVRG: 275361.91175019596\n",
      "SVRG: 275443.3188745744\n",
      "SVRG: 275411.3159381722\n",
      "GD: 281245.58254932665\n",
      "SGD: 257369.4118104499\n",
      "SGD: 280557.19205982145\n",
      "SGD: 281804.68323624675\n",
      "SVRG: 318543.997473789\n",
      "SVRG: 318731.26065958996\n",
      "SVRG: 318689.995796543\n",
      "SVRG: 318446.3440571186\n",
      "SVRG: 318382.49541693233\n",
      "SVRG: 318767.77584659425\n",
      "SVRG: 319326.4612519049\n",
      "SVRG: 319061.18680045346\n",
      "SVRG: 318810.2134151911\n",
      "GD: 318455.46652764676\n",
      "SGD: 406451.0607907339\n",
      "SGD: 288695.75031898014\n",
      "SGD: 336994.26154278737\n",
      "SVRG: 424119.0801103403\n",
      "SVRG: 423419.6828482985\n",
      "SVRG: 423270.55173121375\n",
      "SVRG: 423104.0851197334\n",
      "SVRG: 421473.6247078804\n",
      "SVRG: 423587.91164352704\n",
      "SVRG: 427279.51170144294\n",
      "SVRG: 427796.55687042\n",
      "SVRG: 422119.78668359964\n",
      "GD: 290667.2232265735\n",
      "SGD: 259768.87541971522\n",
      "SGD: 292201.8094234415\n",
      "SGD: 290799.54100381973\n",
      "SVRG: 275419.4095564598\n",
      "SVRG: 275414.0133359298\n",
      "SVRG: 275416.8551739634\n",
      "SVRG: 275413.70092555723\n",
      "SVRG: 275419.84552513843\n",
      "SVRG: 275417.0062921313\n",
      "SVRG: 275372.975255185\n",
      "SVRG: 275415.89027719264\n",
      "SVRG: 275411.34660684754\n",
      "GD: 281245.58254932665\n",
      "SGD: 269738.55129418103\n",
      "SGD: 281696.5348454578\n",
      "SGD: 285124.28876073053\n",
      "SVRG: 318705.91190541367\n",
      "SVRG: 318730.44621477375\n",
      "SVRG: 318635.9643928052\n",
      "SVRG: 318848.03846374276\n",
      "SVRG: 318289.69834107015\n",
      "SVRG: 318767.7494444871\n",
      "SVRG: 318537.300097723\n",
      "SVRG: 318188.25561111805\n",
      "SVRG: 318741.1778242503\n",
      "GD: 318455.46652764676\n",
      "SGD: 278356.6552326049\n",
      "SGD: 304082.0651649035\n",
      "SGD: 323974.590995016\n",
      "SVRG: 422669.67316131137\n",
      "SVRG: 423407.4001157331\n",
      "SVRG: 423428.8172104177\n",
      "SVRG: 421885.1519516059\n",
      "SVRG: 421865.0372541726\n",
      "SVRG: 423640.1426351389\n",
      "SVRG: 429483.77911465085\n",
      "SVRG: 422234.9349520309\n",
      "SVRG: 423748.1185438048\n",
      "init: 535225.0154021684\n",
      "GD: 272856.20588629757\n",
      "SGD: 265566.49350562243\n",
      "SGD: 273457.3173883764\n",
      "SGD: 272204.8024892844\n",
      "SVRG: 276205.673241607\n",
      "SVRG: 276255.4991929789\n",
      "SVRG: 276270.02481027343\n",
      "SVRG: 276224.4988609658\n",
      "SVRG: 276349.16954602854\n",
      "SVRG: 276256.89313897357\n",
      "SVRG: 276514.1808873579\n",
      "SVRG: 276462.5791072257\n",
      "SVRG: 276267.74781762966\n",
      "GD: 304441.8189583528\n",
      "SGD: 396512.9827372518\n",
      "SGD: 311847.1173100933\n",
      "SGD: 320093.1485528158\n",
      "SVRG: 347645.90215246513\n",
      "SVRG: 347129.2439757179\n",
      "SVRG: 347043.3957504968\n",
      "SVRG: 347046.51062667085\n",
      "SVRG: 348033.85476141673\n",
      "SVRG: 346909.4802011162\n",
      "SVRG: 351063.8039010153\n",
      "SVRG: 346627.8753538174\n",
      "SVRG: 347272.1867422722\n",
      "GD: 347469.5992234969\n",
      "SGD: 340962.5909149319\n",
      "SGD: 369616.94046211685\n",
      "SGD: 363664.77169930894\n",
      "SVRG: 377194.90369593323\n",
      "SVRG: 377613.1084315688\n",
      "SVRG: 377370.871341859\n",
      "SVRG: 376702.6184337122\n",
      "SVRG: 377644.203898424\n",
      "SVRG: 377385.1302255791\n",
      "SVRG: 377751.8499961929\n",
      "SVRG: 378735.9182427256\n",
      "SVRG: 377790.92318618234\n",
      "GD: 272856.20588629757\n",
      "SGD: 256166.814002485\n",
      "SGD: 270115.63982173585\n",
      "SGD: 275250.00043198874\n",
      "SVRG: 276334.15831114684\n",
      "SVRG: 276295.1721865374\n",
      "SVRG: 276278.7516527554\n",
      "SVRG: 276183.55712304724\n",
      "SVRG: 276207.524301759\n",
      "SVRG: 276270.0173871578\n",
      "SVRG: 276069.0149164019\n",
      "SVRG: 276152.4140827737\n",
      "SVRG: 276284.6162161149\n",
      "GD: 304441.8189583528\n",
      "SGD: 259364.30339030022\n",
      "SGD: 334788.736163676\n",
      "SGD: 313875.2288618472\n",
      "SVRG: 347555.71548587544\n",
      "SVRG: 347129.60560621973\n",
      "SVRG: 347104.07086393033\n",
      "SVRG: 346309.0434274334\n",
      "SVRG: 347279.25865888264\n",
      "SVRG: 346982.9493632376\n",
      "SVRG: 346397.95292683796\n",
      "SVRG: 347596.8331656656\n",
      "SVRG: 346829.4863787555\n",
      "GD: 347469.5992234969\n",
      "SGD: 327263.5556493906\n",
      "SGD: 387477.2170564039\n",
      "SGD: 361070.3530703187\n",
      "SVRG: 377889.8601972657\n",
      "SVRG: 377073.60755023453\n",
      "SVRG: 377329.66086226417\n",
      "SVRG: 374838.49370368104\n",
      "SVRG: 375569.5278983784\n",
      "SVRG: 377342.27912334196\n",
      "SVRG: 382022.9571484599\n",
      "SVRG: 378524.7642595864\n",
      "SVRG: 377322.9805262424\n",
      "GD: 272856.20588629757\n",
      "SGD: 298866.1516704405\n",
      "SGD: 275909.2213844792\n",
      "SGD: 269840.51063799445\n",
      "SVRG: 276248.5099202997\n",
      "SVRG: 276265.4005802238\n",
      "SVRG: 276269.12781612744\n",
      "SVRG: 276232.8871091976\n",
      "SVRG: 276252.3840726274\n",
      "SVRG: 276273.98636454897\n",
      "SVRG: 276382.9778976434\n",
      "SVRG: 276221.1973046985\n",
      "SVRG: 276320.26448030054\n",
      "GD: 304441.8189583528\n",
      "SGD: 319956.5330265642\n",
      "SGD: 308400.3022848714\n",
      "SGD: 312707.8100069972\n",
      "SVRG: 347163.7500814533\n",
      "SVRG: 346891.8234926769\n",
      "SVRG: 347123.29048205994\n",
      "SVRG: 344709.67248611787\n",
      "SVRG: 346878.7868061486\n",
      "SVRG: 347224.77047501353\n",
      "SVRG: 346966.88219177834\n",
      "SVRG: 346080.41344170825\n",
      "SVRG: 347252.08751184744\n",
      "GD: 347469.5992234969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 364509.79562404874\n",
      "SGD: 377255.0815711035\n",
      "SGD: 357885.0508183019\n",
      "SVRG: 377215.94398201205\n",
      "SVRG: 377602.1987036252\n",
      "SVRG: 377436.09200918017\n",
      "SVRG: 379070.73531186796\n",
      "SVRG: 376635.23746871296\n",
      "SVRG: 377262.6497916462\n",
      "SVRG: 383021.48898906494\n",
      "SVRG: 376781.954849212\n",
      "SVRG: 377093.8504442163\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cost_GD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4ad3a9665ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_GD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cost_GD' is not defined"
     ]
    }
   ],
   "source": [
    "# Tuning hyper parameters:\n",
    "GD_params=[]\n",
    "GD_cost=[]\n",
    "\n",
    "SGD_params=[]\n",
    "SGD_cost=[]\n",
    "\n",
    "SVRG_params=[]\n",
    "SVRG_cost=[]\n",
    "\n",
    "for W_SIZE in [3,5,10]:\n",
    "    while(True):\n",
    "        w_1 = initialize_w(W_SIZE,X_train.shape[0])\n",
    "        w_2 = initialize_w(W_SIZE,W_SIZE)\n",
    "        w_3 = initialize_w(1,W_SIZE)    \n",
    "        initialCost = cost(X_train,y_train,w_1,w_2,w_3,lmbda=0.1)\n",
    "        if(initialCost>300000):\n",
    "            print(\"init:\",initialCost)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    for LAMBDA in [0.01,0.05,0.1]:\n",
    "        for LR in [0.01,0.05,0.1]:\n",
    "            \n",
    "            #GD\n",
    "            GD_params.append([W_SIZE,LAMBDA,LR])\n",
    "            w_1_GD,w_2_GD,w_3_GD,cost_l_GD = GD(X_train, y_train, w_1,w_2,w_3, learning_rate = LR, lmbda=LAMBDA, iterations=50)\n",
    "            GD_cost.append(cost_l_GD)\n",
    "            print(\"GD:\",GD_cost[-1][-1])\n",
    "           \n",
    "            #SGD\n",
    "            for B_SIZE in [2,10,100]:\n",
    "                SGD_params.append([W_SIZE,LAMBDA,LR,B_SIZE])\n",
    "                w_1_SGD,w_2_SGD,w_3_SGD,cost_l_SGD = SGD(X_train, y_train, w_1,w_2,w_3, learning_rate = LR, lmbda=LAMBDA,batch_size=B_SIZE, iterations=50)\n",
    "                SGD_cost.append(cost_l_SGD)\n",
    "                print(\"SGD:\",SGD_cost[-1][-1])   \n",
    "\n",
    "            #SVRG\n",
    "            for T_ in [2,5,10]:\n",
    "                for M_ in [2,10,100]:\n",
    "                    SVRG_params.append([W_SIZE,LAMBDA,LR,T_,M_])\n",
    "                    w_1_SVRG,w_2_SVRG,w_3_SVRG,cost_l_SVRG = SVRG(X_train, y_train, w_1,w_2,w_3, learning_rate = LR, lmbda=LAMBDA,T=T_,M=M_, iterations=10)\n",
    "                    SVRG_cost.append(cost_l_SVRG)\n",
    "                    print(\"SVRG:\",SVRG_cost[-1][-1])                   \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('Hyper', 'wb')\n",
    "# dump information to that file\n",
    "pk.dump((GD_cost,GD_params,SGD_cost,SGD_params,SVRG_cost,SVRG_params), file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVRG_params=[]\n",
    "for W_SIZE in [3,5,10]:\n",
    "\n",
    "    for LAMBDA in [0.01,0.05,0.1]:\n",
    "        for LR in [0.01,0.05,0.1]:\n",
    "\n",
    "            #SVRG\n",
    "            for T_ in [2,5,10]:\n",
    "                for M_ in [2,10,100]:\n",
    "                    SVRG_params.append([W_SIZE,LAMBDA,LR,T_,M_])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
